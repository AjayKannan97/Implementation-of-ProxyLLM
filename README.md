# Implementation-of-ProxyLLM
Implementation of ProxyLLM using LM Studio

## ProxyLLM â€” Local LLM-Powered Customer Support Text Styler

A lightweight, developer-friendly framework that rewrites customer support messages into neutral, polite, or professional tones using locally hosted Large Language Models (LLMs) via LM Studio and Python.

Inspired by the [ProxyLLM](https://github.com/sehyeongjo/Proxy-LLM) research paper, this implementation offers a practical way to integrate AI-driven text rewording directly into customer support workflows without sending sensitive data to external APIs.

---

## ğŸ’¡ Features

- ğŸ§  **Tone Transfer:** Rewrite negative or emotional customer messages into a friendly or neutral tone.
- âš¡ï¸ **Local Inference:** Run fully offline via LM Studio.
- ğŸ” **Sentiment Analysis:** Detects sentiment using NLTK before sending to the LLM.
- ğŸ”Œ **Easy Integration:** Pair with a Chrome Extension or web apps.

---

## ğŸ—ï¸ Architecture Overview

User Input â¡ï¸ Python Backend â¡ï¸ Sentiment Analysis â¬‡ï¸ If Negative â¡ï¸ LM Studio Local LLM â¡ï¸ Rewritten Text â¬‡ï¸ Client or Chrome Extension


---

## ğŸš€ Setup Instructions

### 1ï¸âƒ£ Prerequisites

- Python 3.8+
- LM Studio (Download: https://lmstudio.ai/)
- An LLM model loaded in LM Studio (`Llama3-Instruct` or `Mistral-Instruct` recommended).

---

### 2ï¸âƒ£ Install Dependencies

```bash
pip install flask requests nltk
